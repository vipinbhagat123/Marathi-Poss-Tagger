{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# import requests\n","# from bs4 import BeautifulSoup\n","# import os\n","# import datetime # Import datetime for unique filenames\n","\n","# # Define a User-Agent header to mimic a web browser\n","# headers = {\n","#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n","# }\n","\n","# def scrape_article(url):\n","#     \"\"\"\n","#     Scrapes the title and main article content from a given URL.\n","\n","#     Args:\n","#         url (str): The URL of the article to scrape.\n","\n","#     Returns:\n","#         tuple: A tuple containing (title_text, article_text) if successful,\n","#                otherwise (None, None). Prints error messages on failure.\n","#     \"\"\"\n","#     print(f\"\\nAttempting to fetch content from: {url}\")\n","#     try:\n","#         # Send a GET request to the URL\n","#         response = requests.get(url, headers=headers)\n","#         response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n","\n","#         # Parse the HTML content using BeautifulSoup\n","#         soup = BeautifulSoup(response.text, 'html.parser')\n","\n","#         print(\"Successfully fetched the initial HTML content.\")\n","#         print(\"--- Page Title ---\")\n","#         # Attempt to find the main title of the article\n","#         article_title = soup.find('h1')\n","#         title_text = article_title.get_text(strip=True) if article_title else \"No title found\"\n","#         print(title_text)\n","#         print(\"------------------\")\n","\n","#         # --- Extracting Article Content ---\n","#         print(\"--- Attempting to extract Marathi News Content ---\")\n","\n","#         # Common classes for article body on news websites\n","#         article_body = soup.find('div', class_='article-content') # A common class for article content\n","#         if not article_body:\n","#             article_body = soup.find('div', class_='story-content') # Another common class\n","#         if not article_body:\n","#             article_body = soup.find('div', id='article-body') # Or an ID\n","\n","#         article_text = \"\"\n","#         if article_body:\n","#             # Extract text from all paragraph tags within the identified article body\n","#             paragraphs = article_body.find_all('p')\n","#             article_text = \"\\n\\n\".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n","\n","#             if article_text:\n","#                 print(\"\\nArticle Content Found (first 500 characters):\")\n","#                 print(article_text[:500] + \"...\" if len(article_text) > 500 else article_text)\n","#                 print(\"\\n--- End of Article Content Snippet ---\")\n","#             else:\n","#                 print(\"No readable text paragraphs found within the identified article content area.\")\n","#         else:\n","#             print(\"Could not find a common article content div (e.g., 'article-content', 'story-content', 'article-body').\")\n","#             print(\"Please inspect the website's HTML structure to find the correct selector for this URL.\")\n","\n","#         return title_text, article_text\n","\n","#     except requests.exceptions.HTTPError as errh:\n","#         print(f\"HTTP Error for {url}: {errh}\")\n","#     except requests.exceptions.ConnectionError as errc:\n","#         print(f\"Error Connecting to {url}: {errc}\")\n","#     except requests.exceptions.Timeout as errt:\n","#         print(f\"Timeout Error for {url}: {errt}\")\n","#     except requests.exceptions.RequestException as err:\n","#         print(f\"An unexpected error occurred for {url}: {err}\")\n","#     return None, None\n"],"metadata":{"id":"TZpWQOpKR4vM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    urls_file_name = \"/content/k1.txt\"\n","    output_file_name = \"scraped_articles_combined.txt\" # Define the single output file\n","\n","    if not os.path.exists(urls_file_name):\n","        print(f\"Error: File '{urls_file_name}' not found. Please create a text file with one URL per line.\")\n","    else:\n","        with open(urls_file_name, 'r', encoding='utf-8') as f:\n","            urls = [line.strip() for line in f if line.strip()] # Read non-empty lines\n","\n","        if not urls:\n","            print(f\"The file '{urls_file_name}' is empty or contains no valid URLs.\")\n","        else:\n","            print(f\"\\nFound {len(urls)} URLs in '{urls_file_name}'. Starting scraping...\\n\")\n","\n","            # Open the single output file in append mode\n","            try:\n","                with open(output_file_name, \"a\", encoding=\"utf-8\") as out_f:\n","                    for i, url_to_scrape in enumerate(urls):\n","                        print(f\"\\n--- Processing URL {i+1}/{len(urls)} ---\")\n","                        title, content = scrape_article(url_to_scrape)\n","                        if title and content:\n","                            print(f\"\\nSuccessfully scraped: {title}\")\n","                            # Write to the single output file, separated by title\n","                            out_f.write(f\"{title} \\n\")\n","                            out_f.write(content)\n","                            # out_f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\") # Add a clear separator between articles\n","                        else:\n","                            print(f\"\\nFailed to scrape content from: {url_to_scrape}\")\n","                            out_f.write(f\"--- Failed to scrape from: {url_to_scrape} ---\\n\")\n","                            out_f.write(\"No content extracted.\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n","                print(f\"\\nAll scraped content saved to '{output_file_name}'\")\n","            except IOError as e:\n","                print(f\"Error opening or writing to {output_file_name}: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"987HZ8_cTQyW","executionInfo":{"status":"ok","timestamp":1753218996355,"user_tz":-330,"elapsed":1682,"user":{"displayName":"Vipin Bhagat","userId":"10099796741311419422"}},"outputId":"25967aa2-1012-4dce-a359-d07788344c87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Found 1 URLs in '/content/k1.txt'. Starting scraping...\n","\n","\n","--- Processing URL 1/1 ---\n","\n","Attempting to fetch content from: https://www.lokmat.com/pune/bajrangi-bhaijaan-style-action-15-bangladeshi-women-rescued-from-prostitution-travel-home-a-a301/#google_vignette\n","Successfully fetched the initial HTML content.\n","--- Page Title ---\n","बजरंगी भाईजान’ स्टाईल कारवाई; वेश्या व्यवसायातून सुटका झालेल्या त्या १५ बांगलादेशी महिलांचा मायदेशी प्रवास!\n","------------------\n","--- Attempting to extract Marathi News Content ---\n","\n","Article Content Found (first 500 characters):\n","‘बजरंगी भाईजान’ या चित्रपटात सरहद्द पार करणारी निरागस मुन्नी प्रेक्षकांच्या मनाला भावली होती. मात्र पुण्यात उघडकीस आलेली सरहद्द ओलांडणारी कथा देशाच्या सुरक्षेवर गदा आणणाऱ्या गंभीर बेकायदेशीर कारवायांशी संबंधित ठरली आहे.पुणेशहरात छुप्या पद्धतीने वास्तव्यास असलेल्या १५ बांग्लादेशी  महिलांना पकडण्यात आले होते. त्यानंतर या महिलांना त्यांच्या देशात परत पाठवण्यासाठी पोलिसांनी मोहीम उघडली आहे.\n","\n","गुन्हे शाखेची आठ पथकं, सातत्याने कारवाईगुन्हे शाखेच्या आठ विशेष पथकांनी १५ जुलैपासून शहरात बेकायदेशीरपणे वास्...\n","\n","--- End of Article Content Snippet ---\n","\n","Successfully scraped: बजरंगी भाईजान’ स्टाईल कारवाई; वेश्या व्यवसायातून सुटका झालेल्या त्या १५ बांगलादेशी महिलांचा मायदेशी प्रवास!\n","\n","All scraped content saved to 'scraped_articles_combined.txt'\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","def get_headlines_from_page(url):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","\n","    # Corrected selector for Lokmat\n","    headlines = soup.select('div.card-title a')\n","    return [headline.get_text(strip=True) for headline in headlines]\n","\n","# Corrected base URL (notice the `?page={}` part)\n","base_url = 'https://www.lokmat.com/latestnews/?page={}'\n","\n","num_pages = 5  # You can increase this\n","all_headlines = []\n","\n","for page in range(1, num_pages + 1):\n","    url = base_url.format(page)\n","    print(f\"Scraping Page: {page} -> {url}\")\n","    try:\n","        headlines = get_headlines_from_page(url)\n","        all_headlines.extend(headlines)\n","        time.sleep(1)  # To avoid overloading the server\n","    except Exception as e:\n","        print(f\"Error on page {page}: {e}\")\n","\n","# Save and display\n","df = pd.DataFrame({'Headline': all_headlines})\n","print(df)\n","df.to_csv('lokmat_marathi_headlines.csv', index=False, encoding='utf-8-sig')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gg-4fH97-rMF","executionInfo":{"status":"ok","timestamp":1753219178732,"user_tz":-330,"elapsed":7816,"user":{"displayName":"Vipin Bhagat","userId":"10099796741311419422"}},"outputId":"19c4c1b4-b13a-4781-f083-0db322e3084c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping Page: 1 -> https://www.lokmat.com/latestnews/?page=1\n","Scraping Page: 2 -> https://www.lokmat.com/latestnews/?page=2\n","Scraping Page: 3 -> https://www.lokmat.com/latestnews/?page=3\n","Scraping Page: 4 -> https://www.lokmat.com/latestnews/?page=4\n","Scraping Page: 5 -> https://www.lokmat.com/latestnews/?page=5\n","Empty DataFrame\n","Columns: [Headline]\n","Index: []\n"]}]},{"cell_type":"code","source":["!pip install selenium"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_ZXR3oIA7Cb","executionInfo":{"status":"ok","timestamp":1753219336547,"user_tz":-330,"elapsed":10971,"user":{"displayName":"Vipin Bhagat","userId":"10099796741311419422"}},"outputId":"05862e68-06e8-4b78-85a7-eb9f77e3341a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting selenium\n","  Downloading selenium-4.34.2-py3-none-any.whl.metadata (7.5 kB)\n","Requirement already satisfied: urllib3~=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (2.5.0)\n","Collecting trio~=0.30.0 (from selenium)\n","  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting trio-websocket~=0.12.2 (from selenium)\n","  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n","Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.7.14)\n","Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n","Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n","Collecting outcome (from trio~=0.30.0->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n","Downloading selenium-4.34.2-py3-none-any.whl (9.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n","Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n","Successfully installed outcome-1.3.0.post0 selenium-4.34.2 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"]}]},{"cell_type":"code","source":["from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","# Setup Chrome headless\n","options = Options()\n","options.add_argument('--headless')\n","options.add_argument('--disable-gpu')\n","options.add_argument('--no-sandbox')\n","\n","# Launch driver\n","driver = webdriver.Chrome(options=options)\n","\n","all_headlines = []\n","num_pages = 5\n","\n","for page in range(1, num_pages + 1):\n","    url = f'https://www.lokmat.com/latestnews/?page={page}'\n","    print(f\"Scraping Page {page} -> {url}\")\n","\n","    driver.get(url)\n","    time.sleep(3)  # Wait for JS to load\n","\n","    soup = BeautifulSoup(driver.page_source, 'html.parser')\n","\n","    # Lokmat headlines are in <a class=\"card-title\">...</a>\n","    headline_tags = soup.select('a.card-title')\n","\n","    for tag in headline_tags:\n","        text = tag.get_text(strip=True)\n","        if text:\n","            all_headlines.append(text)\n","\n","driver.quit()\n","\n","# Save and show\n","df = pd.DataFrame({'Headline': all_headlines})\n","print(df)\n","df.to_csv('lokmat_marathi_headlines.csv', index=False, encoding='utf-8-sig')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Yf0rEsRACc6","executionInfo":{"status":"ok","timestamp":1753219541203,"user_tz":-330,"elapsed":20702,"user":{"displayName":"Vipin Bhagat","userId":"10099796741311419422"}},"outputId":"ff7c30e5-9f4f-49ec-c9fe-04ed3ab7ddf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping Page 1 -> https://www.lokmat.com/latestnews/?page=1\n","Scraping Page 2 -> https://www.lokmat.com/latestnews/?page=2\n","Scraping Page 3 -> https://www.lokmat.com/latestnews/?page=3\n","Scraping Page 4 -> https://www.lokmat.com/latestnews/?page=4\n","Scraping Page 5 -> https://www.lokmat.com/latestnews/?page=5\n","Empty DataFrame\n","Columns: [Headline]\n","Index: []\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import os # Import the os module for path manipulation\n","\n","def scrape_lokmat_headlines():\n","    \"\"\"\n","    Scrapes politics headlines from the Lokmat Marathi news website\n","    and saves them to a text file.\n","    \"\"\"\n","    # URL for Lokmat's politics section (found via search)\n","    url = 'https://www.lokmat.com/topics/politics/'\n","    output_filename = 'lokmat_politics_headlines.txt'\n","\n","    try:\n","        # Send a GET request to the URL\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n","\n","        # Parse the HTML content of the page\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        print(f\"--- Scraping Politics Headlines from {url} ---\")\n","\n","        headlines = []\n","\n","        # Lokmat's politics page seems to use <h2> and <h3> tags for headlines,\n","        # often containing an <a> tag for the link and text.\n","        # We'll prioritize these.\n","\n","        # Attempt 1: Look for <h2> tags that might contain headlines\n","        for h2_tag in soup.find_all('h2'):\n","            if h2_tag.a and h2_tag.a.text.strip():\n","                headline_text = h2_tag.a.text.strip()\n","                if headline_text and len(headline_text) > 10: # Filter out very short or empty strings\n","                    headlines.append(headline_text)\n","            elif h2_tag.text.strip() and len(h2_tag.text.strip()) > 10:\n","                headlines.append(h2_tag.text.strip())\n","\n","        # Attempt 2: Look for <h3> tags that might contain headlines\n","        for h3_tag in soup.find_all('h3'):\n","            if h3_tag.a and h3_tag.a.text.strip():\n","                headline_text = h3_tag.a.text.strip()\n","                if headline_text and len(headline_text) > 10:\n","                    headlines.append(headline_text)\n","            elif h3_tag.text.strip() and len(h3_tag.text.strip()) > 10:\n","                headlines.append(h3_tag.text.strip())\n","\n","        # Attempt 3: General links within common news block classes.\n","        # This is a fallback and might include non-headline links, but can catch some.\n","        for news_block in soup.find_all(['div', 'section'], class_=lambda x: x and ('news' in x or 'headline' in x or 'story' in x or 'listing' in x)):\n","            for link in news_block.find_all('a'):\n","                headline_text = link.text.strip()\n","                if headline_text and len(headline_text) > 10 and headline_text not in headlines: # Avoid duplicates\n","                    headlines.append(headline_text)\n","\n","        if not headlines:\n","            print(\"No politics headlines found with the current parsing logic.\")\n","            print(\"The website's structure might have changed, or the selectors need refinement.\")\n","            print(\"Please inspect lokmat.com/topics/politics/ using your browser's developer tools (F12) for updated HTML structure.\")\n","        else:\n","            # Save headlines to a text file\n","            try:\n","                with open(output_filename, 'w', encoding='utf-8') as f:\n","                    for headline in headlines:\n","                        f.write(headline + '\\n')\n","                print(f\"\\nSuccessfully scraped {len(headlines)} headlines.\")\n","                print(f\"Headlines saved to '{output_filename}' in the current directory.\")\n","\n","                # Optionally, print headlines to console as well\n","                print(\"\\n--- Scraped Headlines (first 10) ---\")\n","                for i, headline in enumerate(headlines[:10]):\n","                    print(f\"{i+1}. {headline}\")\n","                if len(headlines) > 10:\n","                    print(f\"... and {len(headlines) - 10} more. Check '{output_filename}' for full list.\")\n","\n","            except IOError as e:\n","                print(f\"Error saving headlines to file: {e}\")\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching the page: {e}\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# Call the function to scrape headlines\n","scrape_lokmat_headlines()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-jcGYjdAuC_","executionInfo":{"status":"ok","timestamp":1753219790853,"user_tz":-330,"elapsed":489,"user":{"displayName":"Vipin Bhagat","userId":"10099796741311419422"}},"outputId":"206f92df-060b-4aeb-e744-c64b6b1becee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Scraping Politics Headlines from https://www.lokmat.com/topics/politics/ ---\n","\n","Successfully scraped 31 headlines.\n","Headlines saved to 'lokmat_politics_headlines.txt' in the current directory.\n","\n","--- Scraped Headlines (first 10) ---\n","1. लाईव्ह न्यूज :\n","2. Politics, Latest Marathi News\n","3. कोल्हापूर :\n","4. राष्ट्रीय :\n","5. राष्ट्रीय :\n","6. कोल्हापूर :\n","7. OUR NETWORK\n","8. महत्वाच्या बातम्या\n","9. आंतरराष्ट्रीय\n","10. Lokmat Games\n","... and 21 more. Check 'lokmat_politics_headlines.txt' for full list.\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import os # Import the os module for path manipulation\n","\n","def scrape_lokmat_headlines():\n","    \"\"\"\n","    Scrapes politics headlines from the Lokmat Marathi news website,\n","    ensuring each headline is on a single line and truncated to 200 characters,\n","    and saves them to a text file.\n","    \"\"\"\n","    # URL for Lokmat's politics section\n","    url = 'https://www.lokmat.com/topics/politics/'\n","    output_filename = 'lokmat_politics_headlines.txt'\n","    max_headline_length = 200 # Define maximum length for a headline\n","\n","    try:\n","        # Send a GET request to the URL\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n","\n","        # Parse the HTML content of the page\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        print(f\"--- Scraping Politics Headlines from {url} ---\")\n","\n","        headlines = set() # Use a set to automatically handle duplicates\n","\n","        # Lokmat's politics page typically uses <h2> and <h3> tags for headlines,\n","        # often containing an <a> tag for the link and text.\n","\n","        # Prioritize finding headlines within common news article structures\n","        # Look for specific div/section classes that typically contain news articles.\n","        # These class names are common patterns, but might need adjustment based on\n","        # Lokmat's current design. Inspect the website using browser developer tools (F12)\n","        # to find the most accurate selectors.\n","\n","        # Example: articles might be within a div with class 'news-listing' or 'article-card'\n","        # This is a more targeted approach than just looking for all h2/h3.\n","        # However, for simplicity and broad coverage, we'll stick to h2/h3 first,\n","        # and then a more general link search within likely containers.\n","\n","        # Attempt 1: Look for <h2> tags that might contain headlines\n","        for h2_tag in soup.find_all('h2'):\n","            headline_text = \"\"\n","            if h2_tag.a and h2_tag.a.text.strip():\n","                headline_text = h2_tag.a.text.strip()\n","            elif h2_tag.text.strip():\n","                headline_text = h2_tag.text.strip()\n","\n","            if headline_text and len(headline_text) > 10: # Filter out very short or empty strings\n","                # Truncate headline to max_headline_length\n","                headlines.add(headline_text[:max_headline_length])\n","\n","        # Attempt 2: Look for <h3> tags that might contain headlines\n","        for h3_tag in soup.find_all('h3'):\n","            headline_text = \"\"\n","            if h3_tag.a and h3_tag.a.text.strip():\n","                headline_text = h3_tag.a.text.strip()\n","            elif h3_tag.text.strip():\n","                headline_text = h3_tag.text.strip()\n","\n","            if headline_text and len(headline_text) > 10:\n","                # Truncate headline to max_headline_length\n","                headlines.add(headline_text[:max_headline_length])\n","\n","        # Attempt 3: General links within common news block classes as a fallback.\n","        # This is less precise but can catch headlines not in h2/h3.\n","        # It's crucial to filter these well.\n","        for news_block in soup.find_all(['div', 'section'], class_=lambda x: x and ('news' in x or 'headline' in x or 'story' in x or 'listing' in x or 'article' in x)):\n","            for link in news_block.find_all('a'):\n","                headline_text = link.text.strip()\n","                # Ensure the text is substantial and not just \"Read More\" or similar\n","                if headline_text and len(headline_text) > 20 and not any(keyword in headline_text.lower() for keyword in [\"read more\", \"अधिक वाचा\", \"पुढे वाचा\", \"फोटो\", \"व्हिडिओ\"]):\n","                    # Truncate headline to max_headline_length\n","                    headlines.add(headline_text[:max_headline_length])\n","\n","        if not headlines:\n","            print(\"No politics headlines found with the current parsing logic.\")\n","            print(\"The website's structure might have changed, or the selectors need refinement.\")\n","            print(\"Please inspect lokmat.com/topics/politics/ using your browser's developer tools (F12) for updated HTML structure.\")\n","        else:\n","            # Convert set to list for consistent ordering (optional, but good for output)\n","            sorted_headlines = sorted(list(headlines))\n","\n","            # Save headlines to a text file\n","            try:\n","                with open(output_filename, 'w', encoding='utf-8') as f:\n","                    for headline in sorted_headlines:\n","                        f.write(headline + '\\n')\n","                print(f\"\\nSuccessfully scraped {len(sorted_headlines)} unique headlines.\")\n","                print(f\"Headlines saved to '{output_filename}' in the current directory.\")\n","\n","                # Optionally, print headlines to console as well\n","                print(\"\\n--- Scraped Headlines (first 10) ---\")\n","                for i, headline in enumerate(sorted_headlines[:50]):\n","                    print(f\"{i+1}. {headline}\")\n","                if len(sorted_headlines) > 10:\n","                    print(f\"... and {len(sorted_headlines) - 10} more. Check '{output_filename}' for full list.\")\n","\n","            except IOError as e:\n","                print(f\"Error saving headlines to file: {e}\")\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching the page: {e}\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# Call the function to scrape headlines\n","scrape_lokmat_headlines()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbVPEWe6CdSI","executionInfo":{"status":"ok","timestamp":1753219946535,"user_tz":-330,"elapsed":474,"user":{"displayName":"Vipin Bhagat","userId":"10099796741311419422"}},"outputId":"c152e851-a402-492e-b8aa-14161c8c6637"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Scraping Politics Headlines from https://www.lokmat.com/topics/politics/ ---\n","\n","Successfully scraped 29 unique headlines.\n","Headlines saved to 'lokmat_politics_headlines.txt' in the current directory.\n","\n","--- Scraped Headlines (first 10) ---\n","1. 'पत्त्यां'मुळे पत्ता कापला जाणार?\n","2. 'हनी ट्रॅप'मध्ये कोण 'अडकणार'?\n","3. CM देवेंद्र फडणवीस\n","4. Jagdeep Dhankhar J P Nadda: जगदीप धनखड यांनी उपराष्ट्रपती पदाचा अचानक राजीनामा दिला. त्यांच्या राजीनाम्यानंतर जे.पी. नड्डांचे राज्यसभेतील विधान चर्चेत आले आहे. त्याचबरोबर उपराष्ट्रपतींनी बैठक बोलावली \n","5. Lokmat Games\n","6. OUR NETWORK\n","7. PM नरेंद्र मोदी\n","8. Politics, Latest Marathi News\n","9. Vice President of India Resigned: संसदेचे पावसाळी अधिवेशन सुरू असतानाच जगदीप धनखड यांनी राजीनाम्याचा बॉम्ब टाकला. पण, कार्यकाळ पूर्ण करण्यापूर्वी उपराष्ट्रपती पदाचा राजीनामा देणारे धनखड हे पहिलेच व्यक\n","10. आंतरराष्ट्रीय\n","... and 19 more. Check 'lokmat_politics_headlines.txt' for full list.\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import os # Import the os module for path manipulation\n","\n","def scrape_lokmat_headlines():\n","    \"\"\"\n","    Scrapes politics headlines from the Lokmat Marathi news website,\n","    ensuring each headline is on a single line and truncated to 200 characters,\n","    and saves them to a text file in the format \"headline\",label.\n","    \"\"\"\n","    # URL for Lokmat's politics section\n","    url = 'https://www.lokmat.com/topics/politics/'\n","    output_filename = 'lokmat_politics_headlines.txt'\n","    max_headline_length = 200 # Define maximum length for a headline\n","    headline_label = \"politics\" # Define the label for all scraped politics headlines\n","\n","    try:\n","        # Send a GET request to the URL\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n","\n","        # Parse the HTML content of the page\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        print(f\"--- Scraping Politics Headlines from {url} ---\")\n","\n","        headlines = set() # Use a set to automatically handle duplicates\n","\n","        # Lokmat's politics page typically uses <h2> and <h3> tags for headlines,\n","        # often containing an <a> tag for the link and text.\n","\n","        # Attempt 1: Look for <h2> tags that might contain headlines\n","        for h2_tag in soup.find_all('h2'):\n","            headline_text = \"\"\n","            if h2_tag.a and h2_tag.a.text.strip():\n","                headline_text = h2_tag.a.text.strip()\n","            elif h2_tag.text.strip():\n","                headline_text = h2_tag.text.strip()\n","\n","            if headline_text and len(headline_text) > 10: # Filter out very short or empty strings\n","                # Truncate headline to max_headline_length\n","                headlines.add(headline_text[:max_headline_length])\n","\n","        # Attempt 2: Look for <h3> tags that might contain headlines\n","        for h3_tag in soup.find_all('h3'):\n","            headline_text = \"\"\n","            if h3_tag.a and h3_tag.a.text.strip():\n","                headline_text = h3_tag.a.text.strip()\n","            elif h3_tag.text.strip():\n","                headline_text = h3_tag.text.strip()\n","\n","            if headline_text and len(headline_text) > 10:\n","                # Truncate headline to max_headline_length\n","                headlines.add(headline_text[:max_headline_length])\n","\n","        # Attempt 3: General links within common news block classes as a fallback.\n","        # This is less precise but can catch headlines not in h2/h3.\n","        # It's crucial to filter these well.\n","        for news_block in soup.find_all(['div', 'section'], class_=lambda x: x and ('news' in x or 'headline' in x or 'story' in x or 'listing' in x or 'article' in x)):\n","            for link in news_block.find_all('a'):\n","                headline_text = link.text.strip()\n","                # Ensure the text is substantial and not just \"Read More\" or similar\n","                if headline_text and len(headline_text) > 20 and not any(keyword in headline_text.lower() for keyword in [\"read more\", \"अधिक वाचा\", \"पुढे वाचा\", \"फोटो\", \"व्हिडिओ\"]):\n","                    # Truncate headline to max_headline_length\n","                    headlines.add(headline_text[:max_headline_length])\n","\n","        if not headlines:\n","            print(\"No politics headlines found with the current parsing logic.\")\n","            print(\"The website's structure might have changed, or the selectors need refinement.\")\n","            print(\"Please inspect lokmat.com/topics/politics/ using your browser's developer tools (F12) for updated HTML structure.\")\n","        else:\n","            # Convert set to list for consistent ordering (optional, but good for output)\n","            sorted_headlines = sorted(list(headlines))\n","\n","            # Save headlines to a text file in the specified format\n","            try:\n","                with open(output_filename, 'w', encoding='utf-8') as f:\n","                    for headline in sorted_headlines:\n","                        # Write in the format \"headline\",label\n","                        f.write(f'\"{headline}\",{headline_label}\\n')\n","                print(f\"\\nSuccessfully scraped {len(sorted_headlines)} unique headlines.\")\n","                print(f\"Headlines saved to '{output_filename}' in the current directory.\")\n","\n","                # Optionally, print headlines to console as well\n","                print(\"\\n--- Scraped Headlines (first 10) ---\")\n","                for i, headline in enumerate(sorted_headlines[:10]):\n","                    print(f'{i+1}. \"{headline}\",{headline_label}')\n","                if len(sorted_headlines) > 10:\n","                    print(f\"... and {len(sorted_headlines) - 10} more. Check '{output_filename}' for full list.\")\n","\n","            except IOError as e:\n","                print(f\"Error saving headlines to file: {e}\")\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching the page: {e}\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# Call the function to scrape headlines\n","scrape_lokmat_headlines()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_fsAAEsVDVEv","executionInfo":{"status":"ok","timestamp":1753249481850,"user_tz":-330,"elapsed":1092,"user":{"displayName":"Kruti","userId":"17219851179197387371"}},"outputId":"a6f5adc8-e5d6-480b-95b9-cac58602762d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Scraping Politics Headlines from https://www.lokmat.com/topics/politics/ ---\n","\n","Successfully scraped 29 unique headlines.\n","Headlines saved to 'lokmat_politics_headlines.txt' in the current directory.\n","\n","--- Scraped Headlines (first 10) ---\n","1. \"'पत्त्यां'मुळे पत्ता कापला जाणार?\",politics\n","2. \"'हनी ट्रॅप'मध्ये कोण 'अडकणार'?\",politics\n","3. \"CM देवेंद्र फडणवीस\",politics\n","4. \"Jagdeep Dhankhar J P Nadda: जगदीप धनखड यांनी उपराष्ट्रपती पदाचा अचानक राजीनामा दिला. त्यांच्या राजीनाम्यानंतर जे.पी. नड्डांचे राज्यसभेतील विधान चर्चेत आले आहे. त्याचबरोबर उपराष्ट्रपतींनी बैठक बोलावली \",politics\n","5. \"Lokmat Games\",politics\n","6. \"OUR NETWORK\",politics\n","7. \"PM नरेंद्र मोदी\",politics\n","8. \"Politics, Latest Marathi News\",politics\n","9. \"Vice President of India Resigned: संसदेचे पावसाळी अधिवेशन सुरू असतानाच जगदीप धनखड यांनी राजीनाम्याचा बॉम्ब टाकला. पण, कार्यकाळ पूर्ण करण्यापूर्वी उपराष्ट्रपती पदाचा राजीनामा देणारे धनखड हे पहिलेच व्यक\",politics\n","10. \"आंतरराष्ट्रीय\",politics\n","... and 19 more. Check 'lokmat_politics_headlines.txt' for full list.\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import os\n","import time # Import the time module for delays\n","\n","def scrape_lokmat_headlines():\n","    \"\"\"\n","    Scrapes politics headlines from the Lokmat Marathi news website,\n","    iterating through multiple pages to gather at least 8000 unique records.\n","    Ensures each headline is on a single line, truncated to 200 characters,\n","    and saved to a text file in the format \"headline\",label.\n","    \"\"\"\n","    base_url = 'https://www.lokmat.com/topics/politics/'\n","    output_filename = 'lokmat_politics_headlines.txt'\n","    max_headline_length = 200\n","    headline_label = \"politics\"\n","    target_headlines = 8000\n","    max_pages_to_check = 500 # Set a reasonable upper limit for pages to prevent infinite loops\n","\n","    all_headlines = set() # Use a set to store unique headlines across all pages\n","    page_num = 1\n","    no_new_headlines_streak = 0 # Counter to detect when no new headlines are found\n","\n","    print(f\"--- Starting to scrape Politics Headlines from {base_url} ---\")\n","    print(f\"Target: {target_headlines} unique headlines.\")\n","\n","    while len(all_headlines) < target_headlines and page_num <= max_pages_to_check:\n","        current_url = f\"{base_url}page/{page_num}/\" if page_num > 1 else base_url\n","        print(f\"\\nFetching page {page_num}... (Current headlines: {len(all_headlines)})\")\n","\n","        try:\n","            response = requests.get(current_url, timeout=10) # Add a timeout for requests\n","            response.raise_for_status()\n","\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            page_headlines_found = 0\n","\n","            # Attempt 1: Look for <h2> tags that might contain headlines\n","            for h2_tag in soup.find_all('h2'):\n","                headline_text = \"\"\n","                if h2_tag.a and h2_tag.a.text.strip():\n","                    headline_text = h2_tag.a.text.strip()\n","                elif h2_tag.text.strip():\n","                    headline_text = h2_tag.text.strip()\n","\n","                if headline_text and len(headline_text) > 10:\n","                    truncated_headline = headline_text[:max_headline_length]\n","                    if truncated_headline not in all_headlines:\n","                        all_headlines.add(truncated_headline)\n","                        page_headlines_found += 1\n","\n","            # Attempt 2: Look for <h3> tags that might contain headlines\n","            for h3_tag in soup.find_all('h3'):\n","                headline_text = \"\"\n","                if h3_tag.a and h3_tag.a.text.strip():\n","                    headline_text = h3_tag.a.text.strip()\n","                elif h3_tag.text.strip():\n","                    headline_text = h3_tag.text.strip()\n","\n","                if headline_text and len(headline_text) > 10:\n","                    truncated_headline = headline_text[:max_headline_length]\n","                    if truncated_headline not in all_headlines:\n","                        all_headlines.add(truncated_headline)\n","                        page_headlines_found += 1\n","\n","            # Attempt 3: General links within common news block classes as a fallback.\n","            for news_block in soup.find_all(['div', 'section'], class_=lambda x: x and ('news' in x or 'headline' in x or 'story' in x or 'listing' in x or 'article' in x)):\n","                for link in news_block.find_all('a'):\n","                    headline_text = link.text.strip()\n","                    if headline_text and len(headline_text) > 20 and not any(keyword in headline_text.lower() for keyword in [\"read more\", \"अधिक वाचा\", \"पुढे वाचा\", \"फोटो\", \"व्हिडिओ\"]):\n","                        truncated_headline = headline_text[:max_headline_length]\n","                        if truncated_headline not in all_headlines:\n","                            all_headlines.add(truncated_headline)\n","                            page_headlines_found += 1\n","\n","            if page_headlines_found == 0:\n","                no_new_headlines_streak += 1\n","                print(f\"No new headlines found on page {page_num}. Streak: {no_new_headlines_streak}\")\n","                if no_new_headlines_streak >= 3: # If no new headlines for 3 consecutive pages, assume end\n","                    print(\"Stopping as no new headlines were found for several consecutive pages.\")\n","                    break\n","            else:\n","                no_new_headlines_streak = 0 # Reset streak if new headlines are found\n","\n","            page_num += 1\n","            time.sleep(1) # Be polite and wait for 1 second between requests\n","\n","        except requests.exceptions.RequestException as e:\n","            print(f\"Error fetching page {page_num} ({current_url}): {e}\")\n","            no_new_headlines_streak += 1 # Treat network errors as no new headlines for streak\n","            if no_new_headlines_streak >= 5: # Allow more retries for network errors\n","                print(\"Stopping due to persistent network errors.\")\n","                break\n","            time.sleep(5) # Wait longer on error before retrying\n","        except Exception as e:\n","            print(f\"An unexpected error occurred on page {page_num}: {e}\")\n","            break # Stop on unexpected errors\n","\n","    if len(all_headlines) < target_headlines:\n","        print(f\"\\nReached end of available content or max pages ({max_pages_to_check}).\")\n","        print(f\"Collected {len(all_headlines)} unique headlines, which is less than the target of {target_headlines}.\")\n","    else:\n","        print(f\"\\nSuccessfully collected at least {target_headlines} unique headlines!\")\n","\n","    # Convert set to list for consistent ordering before saving\n","    sorted_headlines = sorted(list(all_headlines))\n","\n","    # Save headlines to a text file in the specified format\n","    try:\n","        with open(output_filename, 'w', encoding='utf-8') as f:\n","            for headline in sorted_headlines:\n","                f.write(f'\"{headline}\",{headline_label}\\n')\n","        print(f\"\\nAll {len(sorted_headlines)} unique headlines saved to '{output_filename}' in the current directory.\")\n","\n","        # Optionally, print a few headlines to console\n","        print(\"\\n--- Sample Scraped Headlines (first 10) ---\")\n","        for i, headline in enumerate(sorted_headlines[:10]):\n","            print(f'{i+1}. \"{headline}\",{headline_label}')\n","        if len(sorted_headlines) > 10:\n","            print(f\"... and {len(sorted_headlines) - 10} more. Check '{output_filename}' for full list.\")\n","\n","    except IOError as e:\n","        print(f\"Error saving headlines to file: {e}\")\n","\n","# Call the function to scrape headlines\n","scrape_lokmat_headlines()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5wEciKc2DrlX","executionInfo":{"status":"error","timestamp":1753250021662,"user_tz":-330,"elapsed":469714,"user":{"displayName":"Kruti","userId":"17219851179197387371"}},"outputId":"2f087ad9-7120-4dda-ee07-c68c81ef5219"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Starting to scrape Politics Headlines from https://www.lokmat.com/topics/politics/ ---\n","Target: 8000 unique headlines.\n","\n","Fetching page 1... (Current headlines: 0)\n","\n","Fetching page 2... (Current headlines: 29)\n","\n","Fetching page 3... (Current headlines: 38)\n","\n","Fetching page 4... (Current headlines: 46)\n","\n","Fetching page 5... (Current headlines: 55)\n","\n","Fetching page 6... (Current headlines: 63)\n","\n","Fetching page 7... (Current headlines: 71)\n","\n","Fetching page 8... (Current headlines: 79)\n","\n","Fetching page 9... (Current headlines: 89)\n","\n","Fetching page 10... (Current headlines: 97)\n","\n","Fetching page 11... (Current headlines: 105)\n","\n","Fetching page 12... (Current headlines: 113)\n","\n","Fetching page 13... (Current headlines: 121)\n","\n","Fetching page 14... (Current headlines: 129)\n","\n","Fetching page 15... (Current headlines: 137)\n","\n","Fetching page 16... (Current headlines: 146)\n","\n","Fetching page 17... (Current headlines: 154)\n","\n","Fetching page 18... (Current headlines: 162)\n","\n","Fetching page 19... (Current headlines: 170)\n","\n","Fetching page 20... (Current headlines: 178)\n","\n","Fetching page 21... (Current headlines: 186)\n","\n","Fetching page 22... (Current headlines: 194)\n","\n","Fetching page 23... (Current headlines: 202)\n","\n","Fetching page 24... (Current headlines: 210)\n","\n","Fetching page 25... (Current headlines: 218)\n","\n","Fetching page 26... (Current headlines: 227)\n","\n","Fetching page 27... (Current headlines: 235)\n","\n","Fetching page 28... (Current headlines: 243)\n","\n","Fetching page 29... (Current headlines: 251)\n","\n","Fetching page 30... (Current headlines: 259)\n","\n","Fetching page 31... (Current headlines: 268)\n","\n","Fetching page 32... (Current headlines: 276)\n","\n","Fetching page 33... (Current headlines: 284)\n","\n","Fetching page 34... (Current headlines: 292)\n","\n","Fetching page 35... (Current headlines: 300)\n","\n","Fetching page 36... (Current headlines: 308)\n","\n","Fetching page 37... (Current headlines: 316)\n","\n","Fetching page 38... (Current headlines: 324)\n","\n","Fetching page 39... (Current headlines: 332)\n","\n","Fetching page 40... (Current headlines: 340)\n","\n","Fetching page 41... (Current headlines: 348)\n","\n","Fetching page 42... (Current headlines: 356)\n","\n","Fetching page 43... (Current headlines: 364)\n","\n","Fetching page 44... (Current headlines: 372)\n","\n","Fetching page 45... (Current headlines: 379)\n","\n","Fetching page 46... (Current headlines: 387)\n","\n","Fetching page 47... (Current headlines: 395)\n","\n","Fetching page 48... (Current headlines: 403)\n","\n","Fetching page 49... (Current headlines: 411)\n","\n","Fetching page 50... (Current headlines: 419)\n","\n","Fetching page 51... (Current headlines: 427)\n","\n","Fetching page 52... (Current headlines: 435)\n","\n","Fetching page 53... (Current headlines: 443)\n","\n","Fetching page 54... (Current headlines: 451)\n","\n","Fetching page 55... (Current headlines: 459)\n","\n","Fetching page 56... (Current headlines: 467)\n","\n","Fetching page 57... (Current headlines: 475)\n","\n","Fetching page 58... (Current headlines: 483)\n","\n","Fetching page 59... (Current headlines: 491)\n","\n","Fetching page 60... (Current headlines: 499)\n","\n","Fetching page 61... (Current headlines: 507)\n","\n","Fetching page 62... (Current headlines: 515)\n","\n","Fetching page 63... (Current headlines: 523)\n","\n","Fetching page 64... (Current headlines: 531)\n","\n","Fetching page 65... (Current headlines: 539)\n","\n","Fetching page 66... (Current headlines: 547)\n","\n","Fetching page 67... (Current headlines: 555)\n","\n","Fetching page 68... (Current headlines: 563)\n","\n","Fetching page 69... (Current headlines: 571)\n","\n","Fetching page 70... (Current headlines: 579)\n","\n","Fetching page 71... (Current headlines: 587)\n","\n","Fetching page 72... (Current headlines: 595)\n","\n","Fetching page 73... (Current headlines: 603)\n","\n","Fetching page 74... (Current headlines: 611)\n","\n","Fetching page 75... (Current headlines: 619)\n","\n","Fetching page 76... (Current headlines: 627)\n","\n","Fetching page 77... (Current headlines: 635)\n","\n","Fetching page 78... (Current headlines: 643)\n","\n","Fetching page 79... (Current headlines: 651)\n","\n","Fetching page 80... (Current headlines: 659)\n","\n","Fetching page 81... (Current headlines: 667)\n","\n","Fetching page 82... (Current headlines: 675)\n","\n","Fetching page 83... (Current headlines: 683)\n","\n","Fetching page 84... (Current headlines: 691)\n","\n","Fetching page 85... (Current headlines: 699)\n","\n","Fetching page 86... (Current headlines: 707)\n","\n","Fetching page 87... (Current headlines: 715)\n","\n","Fetching page 88... (Current headlines: 723)\n","\n","Fetching page 89... (Current headlines: 732)\n","\n","Fetching page 90... (Current headlines: 740)\n","\n","Fetching page 91... (Current headlines: 748)\n","\n","Fetching page 92... (Current headlines: 756)\n","\n","Fetching page 93... (Current headlines: 764)\n","\n","Fetching page 94... (Current headlines: 772)\n","\n","Fetching page 95... (Current headlines: 780)\n","\n","Fetching page 96... (Current headlines: 788)\n","\n","Fetching page 97... (Current headlines: 796)\n","\n","Fetching page 98... (Current headlines: 804)\n","\n","Fetching page 99... (Current headlines: 812)\n","\n","Fetching page 100... (Current headlines: 820)\n","\n","Fetching page 101... (Current headlines: 828)\n","\n","Fetching page 102... (Current headlines: 836)\n","\n","Fetching page 103... (Current headlines: 844)\n","\n","Fetching page 104... (Current headlines: 852)\n","\n","Fetching page 105... (Current headlines: 860)\n","\n","Fetching page 106... (Current headlines: 868)\n","\n","Fetching page 107... (Current headlines: 877)\n","\n","Fetching page 108... (Current headlines: 885)\n","\n","Fetching page 109... (Current headlines: 893)\n","\n","Fetching page 110... (Current headlines: 901)\n","\n","Fetching page 111... (Current headlines: 909)\n","\n","Fetching page 112... (Current headlines: 917)\n","\n","Fetching page 113... (Current headlines: 925)\n","\n","Fetching page 114... (Current headlines: 933)\n","\n","Fetching page 115... (Current headlines: 941)\n","\n","Fetching page 116... (Current headlines: 949)\n","\n","Fetching page 117... (Current headlines: 957)\n","\n","Fetching page 118... (Current headlines: 965)\n","\n","Fetching page 119... (Current headlines: 973)\n","\n","Fetching page 120... (Current headlines: 981)\n","\n","Fetching page 121... (Current headlines: 989)\n","\n","Fetching page 122... (Current headlines: 997)\n","\n","Fetching page 123... (Current headlines: 1005)\n","\n","Fetching page 124... (Current headlines: 1013)\n","\n","Fetching page 125... (Current headlines: 1021)\n","\n","Fetching page 126... (Current headlines: 1029)\n","\n","Fetching page 127... (Current headlines: 1037)\n","\n","Fetching page 128... (Current headlines: 1045)\n","\n","Fetching page 129... (Current headlines: 1053)\n","\n","Fetching page 130... (Current headlines: 1062)\n","\n","Fetching page 131... (Current headlines: 1070)\n","\n","Fetching page 132... (Current headlines: 1078)\n","\n","Fetching page 133... (Current headlines: 1086)\n","\n","Fetching page 134... (Current headlines: 1094)\n","\n","Fetching page 135... (Current headlines: 1103)\n","\n","Fetching page 136... (Current headlines: 1111)\n","\n","Fetching page 137... (Current headlines: 1119)\n","\n","Fetching page 138... (Current headlines: 1127)\n","\n","Fetching page 139... (Current headlines: 1135)\n","\n","Fetching page 140... (Current headlines: 1143)\n","\n","Fetching page 141... (Current headlines: 1151)\n","\n","Fetching page 142... (Current headlines: 1159)\n","\n","Fetching page 143... (Current headlines: 1167)\n","\n","Fetching page 144... (Current headlines: 1175)\n","\n","Fetching page 145... (Current headlines: 1183)\n","\n","Fetching page 146... (Current headlines: 1191)\n","\n","Fetching page 147... (Current headlines: 1199)\n","\n","Fetching page 148... (Current headlines: 1207)\n","\n","Fetching page 149... (Current headlines: 1215)\n","\n","Fetching page 150... (Current headlines: 1223)\n","\n","Fetching page 151... (Current headlines: 1231)\n","\n","Fetching page 152... (Current headlines: 1239)\n","\n","Fetching page 153... (Current headlines: 1247)\n","\n","Fetching page 154... (Current headlines: 1255)\n","\n","Fetching page 155... (Current headlines: 1263)\n","\n","Fetching page 156... (Current headlines: 1271)\n","\n","Fetching page 157... (Current headlines: 1279)\n","\n","Fetching page 158... (Current headlines: 1287)\n","\n","Fetching page 159... (Current headlines: 1295)\n","\n","Fetching page 160... (Current headlines: 1303)\n","\n","Fetching page 161... (Current headlines: 1311)\n","\n","Fetching page 162... (Current headlines: 1319)\n","\n","Fetching page 163... (Current headlines: 1327)\n","\n","Fetching page 164... (Current headlines: 1335)\n","\n","Fetching page 165... (Current headlines: 1343)\n","\n","Fetching page 166... (Current headlines: 1351)\n","\n","Fetching page 167... (Current headlines: 1359)\n","\n","Fetching page 168... (Current headlines: 1367)\n","\n","Fetching page 169... (Current headlines: 1376)\n","\n","Fetching page 170... (Current headlines: 1384)\n","\n","Fetching page 171... (Current headlines: 1392)\n","\n","Fetching page 172... (Current headlines: 1400)\n","\n","Fetching page 173... (Current headlines: 1408)\n","\n","Fetching page 174... (Current headlines: 1416)\n","\n","Fetching page 175... (Current headlines: 1424)\n","\n","Fetching page 176... (Current headlines: 1432)\n","\n","Fetching page 177... (Current headlines: 1440)\n","\n","Fetching page 178... (Current headlines: 1448)\n","\n","Fetching page 179... (Current headlines: 1456)\n","\n","Fetching page 180... (Current headlines: 1464)\n","\n","Fetching page 181... (Current headlines: 1472)\n","\n","Fetching page 182... (Current headlines: 1480)\n","\n","Fetching page 183... (Current headlines: 1488)\n","\n","Fetching page 184... (Current headlines: 1496)\n","\n","Fetching page 185... (Current headlines: 1504)\n","\n","Fetching page 186... (Current headlines: 1512)\n","\n","Fetching page 187... (Current headlines: 1521)\n","\n","Fetching page 188... (Current headlines: 1529)\n","\n","Fetching page 189... (Current headlines: 1537)\n","\n","Fetching page 190... (Current headlines: 1545)\n","\n","Fetching page 191... (Current headlines: 1553)\n","\n","Fetching page 192... (Current headlines: 1561)\n","\n","Fetching page 193... (Current headlines: 1569)\n","\n","Fetching page 194... (Current headlines: 1577)\n","\n","Fetching page 195... (Current headlines: 1585)\n","\n","Fetching page 196... (Current headlines: 1593)\n","\n","Fetching page 197... (Current headlines: 1601)\n","\n","Fetching page 198... (Current headlines: 1609)\n","\n","Fetching page 199... (Current headlines: 1617)\n","\n","Fetching page 200... (Current headlines: 1626)\n","\n","Fetching page 201... (Current headlines: 1634)\n","\n","Fetching page 202... (Current headlines: 1642)\n","\n","Fetching page 203... (Current headlines: 1650)\n","\n","Fetching page 204... (Current headlines: 1658)\n","\n","Fetching page 205... (Current headlines: 1666)\n","\n","Fetching page 206... (Current headlines: 1674)\n","\n","Fetching page 207... (Current headlines: 1682)\n","\n","Fetching page 208... (Current headlines: 1690)\n","\n","Fetching page 209... (Current headlines: 1698)\n","\n","Fetching page 210... (Current headlines: 1706)\n","\n","Fetching page 211... (Current headlines: 1714)\n","\n","Fetching page 212... (Current headlines: 1722)\n","\n","Fetching page 213... (Current headlines: 1730)\n","\n","Fetching page 214... (Current headlines: 1738)\n","\n","Fetching page 215... (Current headlines: 1746)\n","\n","Fetching page 216... (Current headlines: 1754)\n","\n","Fetching page 217... (Current headlines: 1762)\n","\n","Fetching page 218... (Current headlines: 1770)\n","\n","Fetching page 219... (Current headlines: 1778)\n","\n","Fetching page 220... (Current headlines: 1786)\n","\n","Fetching page 221... (Current headlines: 1794)\n","\n","Fetching page 222... (Current headlines: 1802)\n","\n","Fetching page 223... (Current headlines: 1810)\n","\n","Fetching page 224... (Current headlines: 1818)\n","\n","Fetching page 225... (Current headlines: 1826)\n","\n","Fetching page 226... (Current headlines: 1834)\n","\n","Fetching page 227... (Current headlines: 1842)\n","\n","Fetching page 228... (Current headlines: 1850)\n","\n","Fetching page 229... (Current headlines: 1858)\n","\n","Fetching page 230... (Current headlines: 1866)\n","\n","Fetching page 231... (Current headlines: 1874)\n","\n","Fetching page 232... (Current headlines: 1882)\n","\n","Fetching page 233... (Current headlines: 1890)\n","\n","Fetching page 234... (Current headlines: 1898)\n","\n","Fetching page 235... (Current headlines: 1907)\n","\n","Fetching page 236... (Current headlines: 1915)\n","\n","Fetching page 237... (Current headlines: 1923)\n","\n","Fetching page 238... (Current headlines: 1931)\n","\n","Fetching page 239... (Current headlines: 1939)\n","\n","Fetching page 240... (Current headlines: 1947)\n","\n","Fetching page 241... (Current headlines: 1955)\n","\n","Fetching page 242... (Current headlines: 1963)\n","\n","Fetching page 243... (Current headlines: 1971)\n","\n","Fetching page 244... (Current headlines: 1979)\n","\n","Fetching page 245... (Current headlines: 1987)\n","\n","Fetching page 246... (Current headlines: 1995)\n","\n","Fetching page 247... (Current headlines: 2003)\n","\n","Fetching page 248... (Current headlines: 2011)\n","\n","Fetching page 249... (Current headlines: 2019)\n","\n","Fetching page 250... (Current headlines: 2027)\n","\n","Fetching page 251... (Current headlines: 2035)\n","\n","Fetching page 252... (Current headlines: 2043)\n","\n","Fetching page 253... (Current headlines: 2051)\n","\n","Fetching page 254... (Current headlines: 2059)\n","\n","Fetching page 255... (Current headlines: 2067)\n","\n","Fetching page 256... (Current headlines: 2075)\n","\n","Fetching page 257... (Current headlines: 2083)\n","\n","Fetching page 258... (Current headlines: 2091)\n","\n","Fetching page 259... (Current headlines: 2099)\n","\n","Fetching page 260... (Current headlines: 2107)\n","\n","Fetching page 261... (Current headlines: 2115)\n","\n","Fetching page 262... (Current headlines: 2123)\n","\n","Fetching page 263... (Current headlines: 2131)\n","\n","Fetching page 264... (Current headlines: 2139)\n","\n","Fetching page 265... (Current headlines: 2147)\n","\n","Fetching page 266... (Current headlines: 2155)\n","\n","Fetching page 267... (Current headlines: 2163)\n","\n","Fetching page 268... (Current headlines: 2171)\n","\n","Fetching page 269... (Current headlines: 2179)\n","\n","Fetching page 270... (Current headlines: 2187)\n","\n","Fetching page 271... (Current headlines: 2195)\n","\n","Fetching page 272... (Current headlines: 2203)\n","\n","Fetching page 273... (Current headlines: 2211)\n","\n","Fetching page 274... (Current headlines: 2219)\n","\n","Fetching page 275... (Current headlines: 2227)\n","\n","Fetching page 276... (Current headlines: 2235)\n","\n","Fetching page 277... (Current headlines: 2243)\n","\n","Fetching page 278... (Current headlines: 2251)\n","\n","Fetching page 279... (Current headlines: 2259)\n","\n","Fetching page 280... (Current headlines: 2267)\n","\n","Fetching page 281... (Current headlines: 2275)\n","\n","Fetching page 282... (Current headlines: 2283)\n","\n","Fetching page 283... (Current headlines: 2291)\n","\n","Fetching page 284... (Current headlines: 2299)\n","\n","Fetching page 285... (Current headlines: 2307)\n","\n","Fetching page 286... (Current headlines: 2315)\n","\n","Fetching page 287... (Current headlines: 2323)\n","\n","Fetching page 288... (Current headlines: 2331)\n","\n","Fetching page 289... (Current headlines: 2339)\n","\n","Fetching page 290... (Current headlines: 2347)\n","\n","Fetching page 291... (Current headlines: 2355)\n","\n","Fetching page 292... (Current headlines: 2363)\n","\n","Fetching page 293... (Current headlines: 2371)\n","\n","Fetching page 294... (Current headlines: 2379)\n","\n","Fetching page 295... (Current headlines: 2387)\n","\n","Fetching page 296... (Current headlines: 2395)\n","\n","Fetching page 297... (Current headlines: 2403)\n","\n","Fetching page 298... (Current headlines: 2411)\n","\n","Fetching page 299... (Current headlines: 2419)\n","\n","Fetching page 300... (Current headlines: 2427)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-959591918.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# Call the function to scrape headlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mscrape_lokmat_headlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-2-959591918.py\u001b[0m in \u001b[0;36mscrape_lokmat_headlines\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add a timeout for requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"ZybVMOBMEBIL"},"execution_count":null,"outputs":[]}]}