{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yk_caqL7j6QT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Read the full labeled dataset\n",
        "with open(\"output_3.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Shuffle to randomize\n",
        "random.shuffle(lines)\n",
        "\n",
        "# Split: 80% for train, 20% for test\n",
        "split_idx = int(0.8 * len(lines))\n",
        "train_data = lines[:split_idx]\n",
        "test_gold = lines[split_idx:]\n",
        "\n",
        "# Save train data (for your HMM training)\n",
        "with open(\"train_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(train_data)\n",
        "\n",
        "# Save gold test data (correct tags for accuracy checking)\n",
        "with open(\"test_gold.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(test_gold)\n",
        "\n",
        "# Create test_input.txt: only words, tags removed\n",
        "with open(\"test_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in test_gold:\n",
        "        tokens = line.strip().split()\n",
        "        words = [token.rsplit(\"/\", 1)[0] for token in tokens if \"/\" in token]\n",
        "        f.write(\" \".join(words) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3br0LJqMXSAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08bc477e-9d67-4964-f89c-cfa8d8a6b6b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete. hmmmodel.txt generated.\n",
            "Lexicon contains 63420 unique words.\n",
            "Suffix model contains 8943 unique suffixes.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import math\n",
        "from decimal import *\n",
        "import codecs\n",
        "\n",
        "# --- NEW: Define a constant for Lidstone Smoothing ---\n",
        "# Using a small value like 0.1 is better than add-one smoothing.\n",
        "LAMBDA = Decimal(0.1)\n",
        "\n",
        "tag_list = set()\n",
        "tag_count = {}\n",
        "word_set = set()\n",
        "\n",
        "\n",
        "# --- NEW: Function to build a lexicon of known words and their possible tags ---\n",
        "def build_lexicon(train_data):\n",
        "    lexicon = {}\n",
        "    for sentence in train_data:\n",
        "        for token in sentence:\n",
        "            # Safely split the token\n",
        "            parts = token.rsplit('/', 1)\n",
        "            if len(parts) == 2:\n",
        "                word, tag = parts\n",
        "                word = word.lower()\n",
        "                if word not in lexicon:\n",
        "                    lexicon[word] = set()\n",
        "                lexicon[word].add(tag)\n",
        "    return lexicon\n",
        "\n",
        "\n",
        "# --- NEW: Function to build a model for handling unknown words via suffixes ---\n",
        "def build_suffix_model(train_data, suffix_len=3):\n",
        "    suffix_tag_counts = {}\n",
        "    suffix_totals = {}\n",
        "\n",
        "    # Count suffix-tag co-occurrences\n",
        "    for sentence in train_data:\n",
        "        for token in sentence:\n",
        "            parts = token.rsplit('/', 1)\n",
        "            if len(parts) == 2:\n",
        "                word, tag = parts\n",
        "                word = word.lower()\n",
        "                if len(word) > suffix_len:\n",
        "                    suffix = word[-suffix_len:]\n",
        "                    # Count (suffix, tag) pair\n",
        "                    if suffix not in suffix_tag_counts:\n",
        "                        suffix_tag_counts[suffix] = {}\n",
        "                    suffix_tag_counts[suffix][tag] = suffix_tag_counts[suffix].get(tag, 0) + 1\n",
        "                    # Count total occurrences of the suffix\n",
        "                    suffix_totals[suffix] = suffix_totals.get(suffix, 0) + 1\n",
        "\n",
        "    # Convert counts to probabilities\n",
        "    suffix_probabilities = {}\n",
        "    for suffix, tag_counts in suffix_tag_counts.items():\n",
        "        suffix_probabilities[suffix] = {}\n",
        "        total = suffix_totals[suffix]\n",
        "        for tag, count in tag_counts.items():\n",
        "            suffix_probabilities[suffix][tag] = Decimal(count) / Decimal(total)\n",
        "\n",
        "    return suffix_probabilities\n",
        "\n",
        "\n",
        "def clean_traindata(raw_lines):\n",
        "    cleaned_data = []\n",
        "    for line in raw_lines:\n",
        "        line = line.strip()\n",
        "        tokens = line.split()\n",
        "        valid_tokens = []\n",
        "        for token in tokens:\n",
        "            if '/' in token and token.count('/') == 1:\n",
        "                word, tag = token.split('/')\n",
        "                if word.strip() and tag.strip():\n",
        "                    valid_tokens.append(token)\n",
        "        if valid_tokens:\n",
        "            cleaned_data.append(valid_tokens)\n",
        "    return cleaned_data\n",
        "\n",
        "\n",
        "def parse_traindata():\n",
        "    fin = \"output_3.txt\"\n",
        "    output_file = \"hmmmodel.txt\"\n",
        "\n",
        "    try:\n",
        "        with codecs.open(fin, mode='r', encoding=\"utf-8\") as input_file:\n",
        "            lines = input_file.readlines()\n",
        "        wordtag_list = clean_traindata(lines)\n",
        "        return wordtag_list\n",
        "    except IOError:\n",
        "        with codecs.open(output_file, mode='w', encoding=\"utf-8\") as fo:\n",
        "            fo.write(\"File not found: {}\".format(fin))\n",
        "        sys.exit()\n",
        "\n",
        "\n",
        "def transition_count(train_data):\n",
        "    global tag_list, word_set, tag_count\n",
        "    transition_dict = {}\n",
        "    tag_count.clear()\n",
        "    word_set.clear()\n",
        "    tag_list.clear()\n",
        "\n",
        "\n",
        "    for value in train_data:\n",
        "        previous = \"start\"\n",
        "        for data in value:\n",
        "            i = data[::-1]\n",
        "            word, tag = data.rsplit('/', 1)\n",
        "            word_set.add(word.lower())\n",
        "            tag_list.add(tag)\n",
        "            tag_count[tag] = tag_count.get(tag, 0) + 1\n",
        "            key = previous + \"~tag~\" + tag\n",
        "            transition_dict[key] = transition_dict.get(key, 0) + 1\n",
        "            previous = tag\n",
        "    return transition_dict\n",
        "\n",
        "\n",
        "def transition_probability(train_data):\n",
        "    count_dict = transition_count(train_data)\n",
        "    prob_dict = {}\n",
        "    start_transitions = 0\n",
        "    tag_transitions = {}\n",
        "\n",
        "    for key, count in count_dict.items():\n",
        "        from_tag = key.split(\"~tag~\")[0]\n",
        "        if from_tag == \"start\":\n",
        "            start_transitions += count\n",
        "        else:\n",
        "            tag_transitions[from_tag] = tag_transitions.get(from_tag, 0) + count\n",
        "\n",
        "    for key, count in count_dict.items():\n",
        "        from_tag = key.split(\"~tag~\")[0]\n",
        "        if from_tag == \"start\":\n",
        "            prob_dict[key] = Decimal(count) / Decimal(start_transitions)\n",
        "        else:\n",
        "            prob_dict[key] = Decimal(count) / Decimal(tag_transitions[from_tag])\n",
        "    return prob_dict\n",
        "\n",
        "\n",
        "# --- MODIFIED: Use Lidstone smoothing (add-lambda) ---\n",
        "def transition_smoothing(train_data):\n",
        "    transition_prob = transition_probability(train_data)\n",
        "    total_tags = len(tag_list)\n",
        "    for tag1 in [\"start\"] + list(tag_list):\n",
        "        denominator = tag_count.get(tag1, 0) + (LAMBDA * total_tags)\n",
        "        for tag2 in tag_list:\n",
        "            key = tag1 + \"~tag~\" + tag2\n",
        "            if key not in transition_prob:\n",
        "                # Apply Lidstone smoothing\n",
        "                transition_prob[key] = LAMBDA / denominator\n",
        "    return transition_prob\n",
        "\n",
        "\n",
        "def emission_count(train_data):\n",
        "    count_word = {}\n",
        "    for value in train_data:\n",
        "        for data in value:\n",
        "            i = data[::-1]\n",
        "            word = data[:-i.find(\"/\") - 1].lower()\n",
        "            tag = data.split(\"/\")[-1]\n",
        "            key = word + \"/\" + tag\n",
        "            count_word[key] = count_word.get(key, 0) + 1\n",
        "    return count_word\n",
        "\n",
        "\n",
        "def emission_probability(train_data):\n",
        "    global tag_count\n",
        "    word_count = emission_count(train_data)\n",
        "    emission_prob_dict = {}\n",
        "    for key in word_count:\n",
        "        tag = key.split(\"/\")[-1]\n",
        "        # Smoothing is handled in Viterbi for unknown words,\n",
        "        # here we use raw probability for known words.\n",
        "        emission_prob_dict[key] = Decimal(word_count[key]) / Decimal(tag_count[tag])\n",
        "    return emission_prob_dict\n",
        "\n",
        "\n",
        "# --- MODIFIED: Main training execution block ---\n",
        "# Train and build all models\n",
        "train_data = parse_traindata()\n",
        "transition_model = transition_smoothing(train_data)\n",
        "emission_model = emission_probability(train_data)\n",
        "\n",
        "# --- NEW: Build the lexicon and suffix model for use in the next cell ---\n",
        "lexicon = build_lexicon(train_data)\n",
        "suffix_model = build_suffix_model(train_data)\n",
        "\n",
        "\n",
        "# Save the HMM models (transition and emission) to a file\n",
        "# Note: The new models (lexicon, suffix_model) are kept in memory for the next step.\n",
        "fout = codecs.open(\"hmmmodel.txt\", mode='w', encoding=\"utf-8\")\n",
        "fout.write('Transition Model\\n')\n",
        "for key, value in transition_model.items():\n",
        "    fout.write('%s:%s\\n' % (key, value))\n",
        "\n",
        "fout.write('Emission Model\\n')\n",
        "for key, value in emission_model.items():\n",
        "    fout.write('%s:%s\\n' % (key, value))\n",
        "\n",
        "# --- NEW: Also save tag counts for smoothing in Viterbi ---\n",
        "fout.write('Tag Counts\\n')\n",
        "for key, value in tag_count.items():\n",
        "    fout.write('%s:%s\\n' % (key, value))\n",
        "\n",
        "fout.close()\n",
        "\n",
        "print(\"Training complete. hmmmodel.txt generated.\")\n",
        "print(f\"Lexicon contains {len(lexicon)} unique words.\")\n",
        "print(f\"Suffix model contains {len(suffix_model)} unique suffixes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "K3lcDEGKXT39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63327a6-cc37-44b0-c012-ff638c3fb46d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagging complete. hmmoutput.txt generated.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from decimal import Decimal\n",
        "import codecs\n",
        "\n",
        "# --- NEW: Define LAMBDA here as well for consistency ---\n",
        "LAMBDA = Decimal(0.1)\n",
        "\n",
        "def parse_models():\n",
        "    fin = \"hmmmodel.txt\"\n",
        "    transition_prob = {}\n",
        "    emission_prob = {}\n",
        "    tag_count_model = {}\n",
        "\n",
        "    try:\n",
        "        with codecs.open(fin, mode='r', encoding=\"utf-8\") as input_file:\n",
        "            lines = input_file.readlines()\n",
        "\n",
        "        # Determine sections by finding header lines\n",
        "        try:\n",
        "            emission_start = lines.index(\"Emission Model\\n\")\n",
        "            tag_count_start = lines.index(\"Tag Counts\\n\")\n",
        "        except ValueError:\n",
        "            print(\"Error: Model file format is incorrect. Headers not found.\")\n",
        "            sys.exit()\n",
        "\n",
        "        # Parse Transition Probabilities\n",
        "        for line in lines[1:emission_start]:\n",
        "            line = line.strip()\n",
        "            if ':' in line:\n",
        "                key, value = line.rsplit(':', 1)\n",
        "                transition_prob[key] = Decimal(value)\n",
        "\n",
        "        # Parse Emission Probabilities\n",
        "        word_set = set()\n",
        "        tag_set = set()\n",
        "        for line in lines[emission_start + 1:tag_count_start]:\n",
        "            line = line.strip()\n",
        "            if ':' in line:\n",
        "                key, value = line.rsplit(':', 1)\n",
        "                emission_prob[key] = Decimal(value)\n",
        "                word, tag = key.rsplit('/', 1)\n",
        "                word_set.add(word)\n",
        "                tag_set.add(tag)\n",
        "\n",
        "        # Parse Tag Counts\n",
        "        for line in lines[tag_count_start + 1:]:\n",
        "            line = line.strip()\n",
        "            if ':' in line:\n",
        "                key, value = line.rsplit(':', 1)\n",
        "                tag_count_model[key] = int(value)\n",
        "\n",
        "        tag_list = list(tag_set)\n",
        "        return tag_list, transition_prob, emission_prob, tag_count_model, word_set\n",
        "\n",
        "    except IOError:\n",
        "        print(f\"File not found: {fin}\")\n",
        "        sys.exit()\n",
        "\n",
        "\n",
        "# --- MODIFIED: The Viterbi algorithm now uses the lexicon and suffix model ---\n",
        "def viterbi_algorithm(sentence, tag_list, transition_prob, emission_prob, tag_count, word_set, lexicon, suffix_model):\n",
        "    sentence = sentence.strip()\n",
        "    word_list = sentence.split(\" \")\n",
        "    viterbi_matrix = [{}]\n",
        "    backpointer = [{}]\n",
        "\n",
        "    # --- Step 1: Initialization ---\n",
        "    for tag in tag_list:\n",
        "        tp = transition_prob.get(\"start~tag~\" + tag, LAMBDA)\n",
        "\n",
        "        word_lower = word_list[0].lower()\n",
        "        em_key = word_lower + \"/\" + tag\n",
        "\n",
        "        if em_key in emission_prob:\n",
        "            em = emission_prob[em_key]\n",
        "        else: # Handle unknown word\n",
        "            # Try suffix model first\n",
        "            suffix = word_lower[-3:]\n",
        "            if suffix in suffix_model and tag in suffix_model[suffix]:\n",
        "                em = suffix_model[suffix][tag]\n",
        "            else: # Fallback to Lidstone smoothing\n",
        "                em = LAMBDA / (tag_count.get(tag, 0) + (LAMBDA * len(word_set)))\n",
        "\n",
        "        viterbi_matrix[0][tag] = tp * em\n",
        "        backpointer[0][tag] = \"start\"\n",
        "\n",
        "    # --- Step 2: Recursion ---\n",
        "    for t in range(1, len(word_list)):\n",
        "        viterbi_matrix.append({})\n",
        "        backpointer.append({})\n",
        "        word_lower = word_list[t].lower()\n",
        "\n",
        "        # --- NEW: Use lexicon to prune the tags we consider ---\n",
        "        possible_tags = lexicon.get(word_lower, tag_list)\n",
        "\n",
        "        for tag in possible_tags:\n",
        "            max_prob = Decimal(0)\n",
        "            best_prev_tag = None\n",
        "\n",
        "            em_key = word_lower + \"/\" + tag\n",
        "            if em_key in emission_prob:\n",
        "                em = emission_prob[em_key]\n",
        "            else: # Handle unknown word\n",
        "                suffix = word_lower[-3:]\n",
        "                if suffix in suffix_model and tag in suffix_model[suffix]:\n",
        "                    em = suffix_model[suffix][tag]\n",
        "                else:\n",
        "                    em = LAMBDA / (tag_count.get(tag, 0) + (LAMBDA * len(word_set)))\n",
        "\n",
        "            # Find the best path to the current tag\n",
        "            for prev_tag in viterbi_matrix[t-1]:\n",
        "                tp = transition_prob.get(prev_tag + \"~tag~\" + tag, LAMBDA)\n",
        "                prob = viterbi_matrix[t-1][prev_tag] * tp\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_prev_tag = prev_tag\n",
        "\n",
        "            viterbi_matrix[t][tag] = max_prob * em\n",
        "            backpointer[t][tag] = best_prev_tag\n",
        "\n",
        "    # --- Step 3: Termination ---\n",
        "    best_last_tag = max(viterbi_matrix[-1], key=viterbi_matrix[-1].get)\n",
        "    tag_sequence = [best_last_tag]\n",
        "\n",
        "    # --- Step 4: Backtracking ---\n",
        "    for t in range(len(word_list) - 1, 0, -1):\n",
        "        prev_tag = backpointer[t][tag_sequence[0]]\n",
        "        tag_sequence.insert(0, prev_tag)\n",
        "\n",
        "    return \" \".join(tag_sequence)\n",
        "\n",
        "\n",
        "# --- MODIFIED: Main Test Execution Block ---\n",
        "# Note: The new models (lexicon, suffix_model) are loaded from the previous cell's execution state.\n",
        "tag_list, transition_model, emission_model, tag_count_model, word_set = parse_models()\n",
        "\n",
        "input_file = codecs.open(\"test_input.txt\", mode='r', encoding=\"utf-8\")\n",
        "fout = codecs.open(\"hmmoutput.txt\", mode='w', encoding=\"utf-8\")\n",
        "\n",
        "for sentence in input_file.readlines():\n",
        "    if sentence.strip() == \"\":\n",
        "        continue\n",
        "    # --- MODIFIED: Pass the new models to the Viterbi function ---\n",
        "    path = viterbi_algorithm(sentence, tag_list, transition_model, emission_model, tag_count_model, word_set, lexicon, suffix_model)\n",
        "\n",
        "    words = sentence.strip().split(\" \")\n",
        "    tags = path.strip().split(\" \")\n",
        "\n",
        "    output_line = []\n",
        "    for j in range(len(words)):\n",
        "        output_line.append(f\"{words[j]}/{tags[j]}\")\n",
        "\n",
        "    fout.write(\" \".join(output_line) + \"\\n\")\n",
        "\n",
        "input_file.close()\n",
        "fout.close()\n",
        "\n",
        "print(\"Tagging complete. hmmoutput.txt generated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht25tfbSaNBt",
        "outputId": "ec90533e-52c3-43e8-92e7-a3a88ebf8703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.57%\n"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "\n",
        "def read_tagged_file(filename):\n",
        "    with codecs.open(filename, mode='r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        tag_data = []\n",
        "        for line in lines:\n",
        "            tokens = line.strip().split()\n",
        "            for token in tokens:\n",
        "                if '/' in token:\n",
        "                    word, tag = token.rsplit('/', 1)\n",
        "                    tag_data.append(tag)\n",
        "        return tag_data\n",
        "\n",
        "# Ground truth (gold standard)\n",
        "true_tags = read_tagged_file(\"test_gold.txt\")\n",
        "\n",
        "# Your model's output\n",
        "predicted_tags = read_tagged_file(\"hmmoutput.txt\")\n",
        "\n",
        "# Ensure both lengths match\n",
        "if len(true_tags) != len(predicted_tags):\n",
        "    print(\"Mismatch in token counts between gold and predicted!\")\n",
        "else:\n",
        "    correct = sum(1 for t1, t2 in zip(true_tags, predicted_tags) if t1 == t2)\n",
        "    total = len(true_tags)\n",
        "    accuracy = (correct / total) * 100\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jakQxahskZfy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0771993",
        "outputId": "f5916115-d884-481d-fdc5-e81a8604a4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 18.85%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import codecs\n",
        "\n",
        "def evaluate_accuracy(pred_file=\"hmmoutput.txt\", gold_file=\"test_gold.txt\"):\n",
        "    with codecs.open(pred_file, \"r\", encoding=\"utf-8\") as pf, codecs.open(gold_file, \"r\", encoding=\"utf-8\") as gf:\n",
        "        pred_lines = pf.readlines()\n",
        "        gold_lines = gf.readlines()\n",
        "\n",
        "    correct = total = 0\n",
        "    for pred, gold in zip(pred_lines, gold_lines):\n",
        "        pred_tokens = pred.strip().split()\n",
        "        gold_tokens = gold.strip().split()\n",
        "        for p, g in zip(pred_tokens, gold_tokens):\n",
        "            if p == g:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Run accuracy check\n",
        "evaluate_accuracy()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}